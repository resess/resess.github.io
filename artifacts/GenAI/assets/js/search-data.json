{
  "1": {
    "title": "Artifacts Assessment",
    "content": "The complete rubric we used to assess requirements specifications and ChatGPT prompts is linked below: . Grading Rubric. . | . . Requirements Assessment: . The first tab of the file (‚ÄúCourse-defined Requirements Components‚Äù) lists six requirements specification components defined in the course: actors, use cases, use case diagram, success scenarios, failure scenarios, and non-functional requirements. . The second tab (‚ÄúRequirements Rubric‚Äù) list the requirements quality attribute, listed in the paper and described below in more detail. . Complete and cover all parts of the project goals, providing sufficient information to implement the system When something is part of the project descriptions and goals, there should be a use case for it. E.g., ‚ÄúAdding a friend can be done in one click‚Äù. What about other parts of the use case / other use cases | Cover the course required requirements: 5-6 non-trivial requirements and 4 components: live updates, external API, complex logic, and external authentication service. | . | Consistent, without having contradictory definitions across multiple requirements. E.g., should not use different words for the same use case, failure scenarios should correspond to concrete steps in success scenarios | . | Unambiguous, i.e., free of multiple, potentially conflicting interpretations of a requirement. | Focused, i.e., defined at the right level of detail (not too coarse-grained or too fine-grained). E.g., ‚Äúadd profile item‚Äù, ‚Äúupdate profile item‚Äù, ‚Äúdelete profile item‚Äù are too fine-grained for an ecommerce application. It can be ‚Äúmanage profile‚Äù. Not every button click deserves to be a separate use case | . | Relevant to the project at hand rather than applicable to any project E.g., ‚Äúresponse time under 1 sec‚Äù should explain why 1 sec is applicable for this project (and not 3 or 0.5). | . | Feasible to implement given the time/resource constraints. E.g., ‚Äúthe server should be available 24/7‚Äù | . | Verifiable/Measurable, i.e., where it is possible to clearly define corresponding test cases. E.g., ‚Äúit should perform well‚Äù is not measurable | . | Correctly classified: functional requirements describe functionality that helps meet the needs of its users and stakeholders; Non-functional requirements specify criteria related to system qualities such as performance, reliability, security, and usability E.g., specifying the performance criteria the system must meet, such as response times or throughput is not a functional requirement | ‚ÄúUser should be able to log-in‚Äù is a functional rather than security requirement | . | Well-formatted, with appropriate notations of use case diagram, success and failure scenarios, etc. | . For each quality attribute, we list a set of concrete issues, such as missing or incomplete descriptions, logical or factual errors, and more, identified during artifact evaluation. . Each issue is assigned a severity level from 0 ‚Äì absent, to 1 ‚Äì rare, 2 ‚Äì moderate, 3 ‚Äì abundant. Severity ranks are then added and normalized for each attribute, translated into a 0-5 scale attribute score (the higher the better), based on the conversion criteria provided in the ‚ÄúConversion‚Äù column. Finally, the scores from different attributes are aggregated as the weighted average of attribute scores. The weights used for each attribute are presented in the column ‚ÄúWeight‚Äù. . . Prompt Assessment: . Prompt quality attributes are listed in the paper and summarized in the figure below. . . The third tab of the file above, ‚ÄúPrompt Rubric - How to Ask (H)‚Äù, lists concrete issues for each of the 28 quality factors (7 attributes √ó 4 cross-cutting qualities). As with requirements, when grading, each issue was assessed by severity and converted into a numeric score on 0 to 5 scale (the higher the better). These scores were then averaged across categories to produce an overall score for the ‚ÄúHow to Ask‚Äù (denoted by H). . The fourth tab, ‚ÄúPrompt Rubric - What to Ask (W)‚Äù, lists all requirements components (as in the first tab). In our evaluation, ChatGPT conversations were checked for the presence of at least one request or question related to each of the six requirements components. The resulting ‚ÄúWhat to Ask‚Äù score is the fraction of requirements components students consulted the ChatGPT about out of all 6 components (denoted by H). . Then, the final score is calculated as H x W. .",
    "url": "https://anonymousresearcher2020.github.io/docs/assessment",
    "relUrl": "/docs/assessment"
  },
  "2": {
    "title": "Process A vs. B (RQ1)",
    "content": "Here we provide supplementary data supporting RQ1 results. . . Requirements Quality Results: . Initial Submissions: Table below shows the average score for each requirement attribute for initial submissions (denoted by H) in processes A+ and A-. . Req. Quality Metric A+ A- . Complete | 3.1 | 3.3 | . Consistent | 3.5 | 3.3 | . Unambiguous | 3.5 | 3.3 | . Focused | 3.4 | 3.3 | . Relevant | 3.2 | 3.1 | . Feasible | 4.8 | 5.0 | . Verifiable/Measurable | 4.8 | 4.1 | . Correctly classified | 4.7 | 4.2 | . Well-formatted | 3.9 | 2.6 | . Total Quality (H) | 77% | 72% | . ChatGPT‚Äôs Outputs: Table below shows the average score for each requirement attribute for ChatGPT‚Äôs outputs (denoted by G) in processes A+, A-, and B. . Req. Quality Metric A+ A- B . Complete | 4.1 | 4.6 | 4.2 | . Consistent | 2.6 | 1.7 | 3.2 | . Unambiguous | 2.9 | 2.9 | 3.6 | . Focused | 3.9 | 3.7 | 3.6 | . Relevant | 3.2 | 3 | 2.8 | . Feasible | 2.7 | 2.3 | 2.3 | . Verifiable/Measurable | 2.9 | 1.4 | 3.0 | . Correctly classified | 4.6 | 4.43 | 4.5 | . Well-formatted | 1.6 | 0.3 | 3.5 | . Total Quality (G) | 63% | 54% | 68% | . Final Submissions: Table below shows the average score for each requirement attribute for final submissions (denoted by F) in processes A+, A-, and B. . Req. Quality Metric A+ A- B . Complete | 4.7 | 4.7 | 4.6 | . Consistent | 4.2 | 3.2 | 4.3 | . Unambiguous | 3.5 | 3.1 | 3.3 | . Focused | 4.2 | 3.7 | 4.1 | . Relevant | 3.3 | 3 | 2.8 | . Feasible | 4.5 | 4.14 | 3.5 | . Verifiable/Measurable | 4.5 | 3.8 | 3.6 | . Correctly classified | 4.6 | 4.5 | 4.5 | . Well-formatted | 3.6 | 3.5 | 3.8 | . Total Quality (F) | 82% | 75% | 77% | . . Prompt Quality Results: . Table below shows the average score for each prompt quality metric in processes A+, A-, and B. . Prompt Quality Metric A+ A- B . Course Setup | 1.1 | 0.6 | 1.9 | . Project Setup | 2.8 | 1.0 | 3.0 | . Explicit Requests | 3.8 | 3.3 | 3.9 | . Expected Content | 3.0 | 2.3 | 4.4 | . Expected Format | 1.9 | 0.9 | 3.3 | . Personas | 0.5 | 0.3 | 0.5 | . Examples | 0.2 | 0 | 0.3 | . Avg. ‚ÄúHow to ask‚Äù | 1.9 | 1.2 | 2.4 | . What to ask | 4 | 3.6 | 3.9 | . Total Quality | 30% | 17% | 38% | . . The raw data obtained in this study cannot be shared because of privacy issues. .",
    "url": "https://anonymousresearcher2020.github.io/docs/assessment_data",
    "relUrl": "/docs/assessment_data"
  },
  "3": {
    "title": "Course Setup",
    "content": "We now specify how the requirements were defined in the scope of this course. The requirements topic was covered in three lectures and two lab sessions. There were two requirements milestones (project idea and final requirement specifications) and an in-lab work (initial submission). . . Requirements . Requirement Engineering Process: In this milestone, we focused on requirements elicitation, analysis, specification (not validation and maintenance). Elicitation: researching and discovering the requirements of a system from users, customers, and other stakeholders. Think about why users want to do something in your app, not just what. | Analysis: critically assessing, refining, and modifying the gathered requirements | Specification: documenting the elicited requirements in a formal manner to ensure clarity, consistency, and completeness. | . | . | Functional Requirements: Describe the system functionality, i.e., what it should do. The use case diagram provides a graphical representation of the system‚Äôs intended interactions with its environment and users, making the system‚Äôs functionality more understandable. It is used to communicate what a system is intended to accomplish: Actors: people, devices, or other systems interact with the system and are external to the system itself. | Use cases: behavior of the software that meets the user‚Äôs need and describe all possible interactions with the system, represented by verbs | Line associations: connect an actor to a use case in which that actor participates | Relationship between actors: Generalization | Relationship between use cases: Generalization, Include, and Exclude | . | The formal requirement specification is the final product that is used to document the requirements in both a graphical and textual format. The lightweight use case specification of each requirement includes: Name, Short description, and Primary Actor(s) | Success Scenarios: normal flow of events in the system. People can relate to these more readily than abstract statements of what they require from a system | Failure Scenarios: what can go wrong corresponding to each step in the success scenario and how this is handled | . | . | . | . | Non-Functional Requirements: Describe system properties and constraints as a whole. Specify acceptance criteria related to system qualities such as performance, reliability, and usability. Performance, safety, security, scalability, dependability, reusability, portability | Specification of each requirement includes: Textual description, Justification (why needed), and Validation approach (how to confirm). | . | . | . . Milestones . Project Proposal Assignment to Produce ‚ÄúProject Idea (I)‚Äù . | Initial Requirements Lab Activity to Produce ‚ÄúInitial Submission (H)‚Äù . | Final Requirement Specification Assignment to Produce ‚ÄúFinal submission (F)‚Äù . | .",
    "url": "https://anonymousresearcher2020.github.io/docs/course_setup",
    "relUrl": "/docs/course_setup"
  },
  "4": {
    "title": "Home",
    "content": "Hey, ChatGPT, Look at MyWork: Using Conversational AI in Requirements Engineering Education . . The emergence of conversational AI tools in late 2022 practically changed the face of software engineering and software engineering education. Contemplating the question of how to best prepare and evaluate students in this new reality, we experimented with systematically introducing a conversational AI tool, ChatGPT, into the 2023 offering of an upper-level undergraduate project-based course on Software Engineering. In this course, 20 groups of four students each had to design and implement a project of their choice, with an Android-based mobile client and a Node.js-based cloud server. This paper discusses our goals, approach, and lessons learned from introducing ChatGPT into the first phase of the project development: scoping the work and defining the project requirements. . Our experience shows that students can achieve comparable results using a variety of ChatGPT interaction modes and the success of each mode largely depends on students‚Äô preferences, learning styles, and the invested effort. Yet, in any of the modes, with moderate effort, students can produce artifacts of a mid-range quality level of around 80%. Moving above this range requires substantial investment, which can be spent on brainstorming, crafting high-quality prompts, or critically assessing ChatGPT‚Äôs output. We also observe low prompting proficiency of the students: students can improve their prompting strategies by providing a more adequate description of their course and project setup, examples, and expected output format for their requests. Interestingly, students can often be ‚Äúswayed‚Äù by ChatGPT‚Äôs projected confidence, even when their original ideas are, in fact, more appropriate than the proposed refinements. We hope that our experience and lessons learned will help spark further discussions on how to best embed AI tools into the software engineering curriculum and work practices. . . This website provides additional information about the course setup in general and the requirements milestone in particular. It also includes the details about our requirements and prompt assessment metrics. Finally, it provides additional supporting data for each research question. We hope this information will enable reproducibility and will inspire further research in this area. .",
    "url": "https://anonymousresearcher2020.github.io/",
    "relUrl": "/"
  },
  "5": {
    "title": "Investigator Data (RQ2)",
    "content": "Prompt templates used by the investigators for both Process A and B are given below. While we cannot share the chats that used the student data for privacy reasons, we created two fictitious project descriptions that closely resemble student data, to illustrate the ChatGPT outputs generated during our case studies. . Process A . | Process B . | Chat Example ‚Äì Process A . | Chat Example ‚Äì Process B . | .",
    "url": "https://anonymousresearcher2020.github.io/docs/investigator_data",
    "relUrl": "/docs/investigator_data"
  },
  "6": {
    "title": "Usage Data (RQ3)",
    "content": "ChatGPT Usage Patterns . We identified three main modes of ChatGPT usage among students. Table below shows the distribution of different types of usage patterns across different processes: ùê¥+, ùê¥‚àí, and ùêµ. . . Each usage mode can be applied to one or more components of the requirements specification: use cases (UC), actors (AC), use case diagram (UCD), success/failure scenarios (SC), and non-functional requirements (NF). We further illustrate the distribution of these components for each usage pattern within each process. . Generation . . Concretization . . Rewriting . .",
    "url": "https://anonymousresearcher2020.github.io/docs/usage_data",
    "relUrl": "/docs/usage_data"
  }
  
}
