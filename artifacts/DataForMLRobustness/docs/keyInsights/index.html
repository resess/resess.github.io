<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=Edge">
        <title>Home - It Is All About Data</title>
        <style>
            .img-container {
              text-align: center;
            }
        </style>
        <meta name="Description" content="It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness">
        <link rel="stylesheet" href="../../assets/css/just-the-docs.css">
        <script type="text/javascript" src="../../assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="../../assets/js/just-the-docs.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Home | Data for ML Robustness</title>
        <meta name="generator" content="Jekyll v3.8.6" />
        <meta property="og:title" content="Home" />
        <meta property="og:locale" content="en_US" />
        <meta name="description" content="It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness" />
        <meta property="og:description" content="It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness" />
        <link rel="canonical" href="../../index.html" />
        <meta property="og:url" content="../../index.html" />
        <meta property="og:site_name" content="Data for ML Robustness" />
        <script type="application/ld+json"> {"url":"../../index.html","description":"It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness.","name":"Data Quality for ML Adversarial Robustness","headline":"Home","@type":"WebSite","@context":"https://schema.org"}</script>
    <body>
        <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
            <symbol id="link" viewBox="0 0 16 16">
                <title>Link</title>
                <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
            </symbol>
        </svg>
        <div class="page-wrap">
        <div class="side-bar">
         <div class="site-header">
            <div id="texts" style="display:inline;">
                <a href="../../index.html" class="site-title lh-tight"> <span style="font-variant:small-caps;font-family:'Times New Roman', Times, serif;">Data for ML Robustness</span> </a> <button class="menu-button fs-3 js-main-nav-trigger" data-text-toggle="Hide" type="button">Menu</button>
            </div>
            <!-- <div id="image" style="display:inline;">
                <img src="../../img/mandoline_Logo_noname.png" alt = "logo" width="50" height="50" />
            </div> -->
        </div>
            <div class="navigation main-nav js-main-nav">
                <nav role="navigation" aria-label="Main navigation">
                    <ul class="navigation-list">
                        <li class="navigation-list-item"><a href="../../index.html" class="navigation-list-link">Home</a>
                        <li class="navigation-list-item"><a href="../../docs/background/index.html" class="navigation-list-link">Background</a>
                        <li class="navigation-list-item"><a href="../../docs/literatureCollection/index.html" class="navigation-list-link">Literature Collection</a>
                        <li class="navigation-list-item"><a href="../../docs/literatureCategorization/index.html" class="navigation-list-link">Literature Categorization</a>
                        <li class="navigation-list-item active"><a href="../../docs/keyInsights/index.html" class="navigation-list-link active">Key Insights</a>
                        <li class="navigation-list-item"><a href="../../docs/relatedWork/index.html" class="navigation-list-link">Additional Related Work</a>
                    </ul>
                    </ul>
                </nav>
            </div>
            <footer class="site-footer">
                <p class="text-small text-grey-dk-000 mb-0">Online Appendix</p>
            </footer>
        </div>
        <div class="main-content-wrap js-main-content" tabindex="0">
        <div class="main-content">
        <div class="page">
        <div id="main-content" class="page-content" role="main">

        <h1 id="keyInsights">Key Insights</h1>
        <div class="scroll-container">
            <img src="../../img/KeyInsights_part1.png" alt="Key insights from the survey - part 1." style="width: 820px; height: 340px;">
            <img src="../../img/KeyInsights_part2.png" alt="Key insights from the survey - part 2." style="width: 820px; height: 340px;">
        </div>

        <div>
            <p>
                <span style="font-family: Arial, Helvetica, sans-serif; font-weight: bold;">Number of samples</span>: 
                More training samples are needed for robust than for standard generalization.
                For a variety of training setups (i.e., different types of classifiers and data distributions), the number of training samples required to achieve robust generalization is proportional to the dimensionality of the training data. 
                Unlabeled samples or generated data can be used to fulfill the need for more samples needed for robust generalization, i.e., to close the sample complexity gap.
                Class imbalance, i.e., having an imbalanced number of samples across different classes, hurts robust generalization due to the model bias towards over-represented samples. 
                Re-weighted loss functions can be used to mitigate this model bias and thereby improve robustness.
            </p>
            <p>
                <span style="font-family: Arial, Helvetica, sans-serif; font-weight: bold;">Dimensionality</span>: 
                Dimensionality captures the size of the feature set.
                Higher dimensionality correlates with higher adversarial risk, worse standard-to-adversarial risk trade-off, difficulty in robustness certification, and difficulty in applying common defense techniques.
                This is because adversarial attacks can exploit the excess dimensions to construct adversarial examples. 
            </p>
            <p>
                <span style="font-family: Arial, Helvetica, sans-serif; font-weight: bold;">Types and Properties of Distribution</span>: 
                Some data distributions are more robust than others, e.g., mixtures of Bernoulli distributions are more robust than mixtures of Gaussian distributions.
                Learning feature representations that resemble robust distributions can improve robustness.
            </p>
            <p>
                <span style="font-family: Arial, Helvetica, sans-serif; font-weight: bold;">Density</span>: 
                Density reflects the closeness of samples in a particular bounded region (intra-class distance).
                Adversarial examples are commonly found in low-density regions of data, where samples are far apart from each other.
                This is because models cannot accurately learn decision boundaries near low-density regions due to the small number of samples available. 
                As such, high data density for each class correlates with lower adversarial risk.
            </p>
            <p>
                <span style="font-family: Arial, Helvetica, sans-serif; font-weight: bold;">Separation</span>: 
                Separation characterizes the distance of samples from different classes to each other (inter-class distance).
                Greater separation between classes decreases adversarial risk as it is harder to generate perturbations that 
                will cross the decision boundaries between classes.
                Most papers that provide techniques for improving separations do so by modifying the loss function to learn a latent data representation with higher separation.
                They also ensure that this increase in separation does not come at the expense of decreasing density, as these two concepts are closely related.
            </p>
            <p>
                <span style="font-family: Arial, Helvetica, sans-serif; font-weight: bold;">Concentraion</span>: 
                Given a function defined over a non-empty set, concentration (from the phenomenon of concentration of measure) is the minimum value of the function after expanding the input set by epsilon in all dimensions.
                For example, expanding the set of misclassified samples by a certain epsilon gives a set of possible samples that can be
                misclassified with an epsilon-size perturbation (candidate adversarial examples).
                Concentration, in this case, measures the minimal possible size of this set, which provides the upper bound of the achievable model robustness.
                As some datasets tend to exhibit inherently high concentration,
                e.g., datasets that lie on unit hypersphere,
                achieving high robust generalization is harder for these datasets.
                The impact of high concentration on adversarial robustness is further magnified for high-dimensional data.
            </p>
            <p>
                <span style="font-family: Arial, Helvetica, sans-serif; font-weight: bold;">Label Quality</span>: 
                High label noise correlates with higher adversarial risk.
                Label noise can be automatically generated as part of adversarial training 
                because perturbation can change the semantics of the perturbed data samples. 
                Labels thus have to be carefully examined and rectified.
                More specific labels, e.g., "cat" and "dog" instead of "animal", are more robust than coarse labels, as they allow the model to extract more distinct features.
                Learning for different tasks concurrently, e.g., to simultaneously locate and estimate the distance of objects in images,improves the robustness of the learned models, as the model can utilize the information from multiple sources of data.
            </p>
            <p>
                <span style="font-family: Arial, Helvetica, sans-serif; font-weight: bold;">Frequency</span>: 
                Image frequency, i.e., the rate of change in pixel value, is shown to be correlated with robustness.
                As humans perceive differences in images by focusing on low-frequency components 
                whereas models consider both low and high-frequency components,  
                high-frequency perturbations can ``fool'' models while being imperceptible to humans. 
                Adversarial training tends to focus on high-frequency components and, as a result,
                adversarially trained models rely more heavily on low-frequency components. 
                While this helps ensure that models focus on low-frequency components as well, it introduces new issues
                related to clean and robust accuracy. 
                Encouraging a diverse distribution of frequencies in training data results in lower adversarial risk.
            </p>
        </div>
    </body>

